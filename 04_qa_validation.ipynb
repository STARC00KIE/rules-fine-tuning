{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c23b4c4",
   "metadata": {},
   "source": [
    "# 04. 생성한 합성 데이터셋 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f2ecb",
   "metadata": {},
   "source": [
    "## Step 1. 통합 및 의미론적 중복 제거 (Semantic Deduplication)\n",
    "### 목표: 생성된 파일에 흩어진 '같은 질문'을 찾아 하나만 남기고 삭제\n",
    "\n",
    "### 방법:\n",
    "\n",
    "1. 파일 병합 (Total Pool 생성).\n",
    "\n",
    "2. 임베딩(Embedding) 기술로 질문 간의 유사도 측정(유사도가 0.85 이상인 질문들을 같은 그룹으로 묶고, 그중 답변 품질이 가장 좋은 1개만 남기고 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73accc0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. 데이터 로드 및 병합 (Data Loading)\n",
    "file_paths = {\n",
    "    0.1: 'qa_dataset_temp_0.1.json',\n",
    "    0.2: 'qa_dataset_temp_0.2.json',\n",
    "    0.3: 'qa_dataset_temp_0.3.json',\n",
    "    0.4: 'qa_dataset_temp_0.4.json'\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "for temp, path in file_paths.items():\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        for entry in data:\n",
    "            entry['temperature'] = temp # 출처 표시\n",
    "        all_data.extend(data)\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# 2. 임베딩 및 유사도 계산 (Embedding & Similarity)\n",
    "# TF-IDF를 사용하여 텍스트를 벡터로 변환\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['question'])\n",
    "\n",
    "# 코사인 유사도 행렬 계산 (질문 간 유사도)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 3. 중복 제거 및 선별 (Deduplication & Selection)\n",
    "visited = set()\n",
    "deduplicated_data = []\n",
    "threshold = 0.85 # 유사도 임계값 (0.85 이상이면 같은 질문으로 간주)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    \n",
    "    # 현재 질문과 유사한 질문들의 인덱스 찾기\n",
    "    similar_indices = np.where(cosine_sim[i] > threshold)[0]\n",
    "    \n",
    "    # 유사한 질문들을 하나의 클러스터로 묶음\n",
    "    cluster = df.iloc[similar_indices].to_dict('records')\n",
    "    \n",
    "    # 방문 처리 (중복 처리 방지)\n",
    "    for idx in similar_indices:\n",
    "        visited.add(idx)\n",
    "    \n",
    "    # [전략] 클러스터 내에서 'Temperature'가 가장 낮은(0.1) 데이터를 대표로 선정\n",
    "    best_candidate = sorted(cluster, key=lambda x: x['temperature'])[0]\n",
    "    deduplicated_data.append(best_candidate)\n",
    "\n",
    "# 4. 결과 저장\n",
    "output_df = pd.DataFrame(deduplicated_data)\n",
    "output_df.to_json('deduplicated_dataset.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"제거 전: {len(df)}개 -> 제거 후: {len(output_df)}개\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

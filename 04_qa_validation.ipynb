{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c23b4c4",
   "metadata": {},
   "source": [
    "# 04. 생성한 합성 데이터셋 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f2ecb",
   "metadata": {},
   "source": [
    "## Step 1. 통합 및 의미론적 중복 제거 (Semantic Deduplication)\n",
    "### 목표: 생성된 파일에 흩어진 '같은 질문'을 찾아 하나만 남기고 삭제\n",
    "\n",
    "### 방법:\n",
    "\n",
    "1. 파일 병합 (Total Pool 생성).\n",
    "\n",
    "2. 임베딩(Embedding) 기술로 질문 간의 유사도 측정(유사도가 0.85 이상인 질문들을 같은 그룹으로 묶고, 그중 답변 품질이 가장 좋은 1개만 남기고 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d73accc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 수: 1405\n",
      "중복 제거 후: 835\n",
      "발견된 중복 그룹 수: 292\n",
      "파일 저장 완료:\n",
      "   1. deduplicated_dataset.jsonl (라인별)\n",
      "   2. deduplicated_dataset.json (리스트형)\n",
      "   3. duplicate_groups_check.json (중복그룹확인)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## option 1\n",
    "- 띄어쓰기 단위로 임베딩하는 스크립트\n",
    "- 질문만 임베딩하여 유사도 계산\n",
    "- 한글 코사인 유사도가 잘 측정되지 않음\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. 데이터 로드 및 병합 (Data Loading)\n",
    "file_paths = {}\n",
    "\n",
    "# 0.0부터 0.4까지 (range(5))\n",
    "for i in range(5): \n",
    "    temp_val = i / 10\n",
    "    filename = f'./data/QA/qwen3-coder-A3B-instruct/qa_dataset_temp_{temp_val:.1f}.json'\n",
    "    file_paths[temp_val] = filename\n",
    "\n",
    "all_data = []\n",
    "for temp, path in file_paths.items():\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            for entry in data:\n",
    "                entry['temperature'] = temp # 출처(Temp) 표시\n",
    "            all_data.extend(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ 경고: 파일을 찾을 수 없습니다 - {path}\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# 2. 임베딩 및 유사도 계산 (Embedding & Similarity)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['question']) # 질문만 임베딩 진행\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 3. 중복 제거 및 선별\n",
    "visited = set()\n",
    "deduplicated_data = []\n",
    "duplicate_groups = []  # 확인용 리스트\n",
    "\n",
    "threshold = 0.85 \n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    \n",
    "    similar_indices = np.where(cosine_sim[i] > threshold)[0]\n",
    "    cluster = df.iloc[similar_indices].to_dict('records')\n",
    "    \n",
    "    # 중복 그룹 저장 (클러스터 크기가 1보다 큰 경우)\n",
    "    if len(cluster) > 1:\n",
    "        group_info = {\n",
    "            \"group_id\": len(duplicate_groups) + 1,\n",
    "            \"count\": len(cluster),\n",
    "            \"questions\": cluster\n",
    "        }\n",
    "        duplicate_groups.append(group_info)\n",
    "    \n",
    "    for idx in similar_indices:\n",
    "        visited.add(idx)\n",
    "    \n",
    "    # 대표 질문 선정 (Temperature가 가장 낮은 것)\n",
    "    best_candidate = sorted(cluster, key=lambda x: x['temperature'])[0]\n",
    "    deduplicated_data.append(best_candidate)\n",
    "\n",
    "# 4. 결과 저장\n",
    "output_df = pd.DataFrame(deduplicated_data)\n",
    "\n",
    "# 4-1. JSONL 포맷 저장 (한 줄에 하나씩)\n",
    "output_df.to_json('deduplicated_dataset.jsonl', orient='records', lines=True, force_ascii=False, indent=4)\n",
    "\n",
    "# 4-2. JSON 리스트 포맷 저장 (전체가 [ ]로 감싸짐)\n",
    "output_df.to_json('deduplicated_dataset.json', orient='records', lines=False, force_ascii=False, indent=4)\n",
    "\n",
    "# 4-3. 중복 그룹 검토용 저장\n",
    "with open('duplicate_groups_check.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(duplicate_groups, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# 5. 요약 출력\n",
    "print(f\"전체 데이터 수: {len(df)}\")\n",
    "print(f\"중복 제거 후: {len(output_df)}\")\n",
    "print(f\"발견된 중복 그룹 수: {len(duplicate_groups)}\")\n",
    "print(f\"파일 저장 완료:\")\n",
    "print(f\"   1. deduplicated_dataset.jsonl (라인별)\")\n",
    "print(f\"   2. deduplicated_dataset.json (리스트형)\")\n",
    "print(f\"   3. duplicate_groups_check.json (중복그룹확인)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e60503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 수: 1405\n",
      "중복 제거 후: 621\n",
      "발견된 중복 그룹 수: 336\n",
      "파일 저장 완료:\n",
      "   1. deduplicated_dataset.jsonl (라인별)\n",
      "   2. deduplicated_dataset.json (리스트형)\n",
      "   3. duplicate_groups_check.json (중복그룹확인)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## option 2\n",
    "- 형태소 단위로 임베딩하는 스크립트\n",
    "- 질문만 임베딩하여 유사도 계산\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 1. 데이터 로드 및 병합 (Data Loading)\n",
    "file_paths = {}\n",
    "\n",
    "# 0.0부터 0.4까지 (range(5))\n",
    "for i in range(5): \n",
    "    temp_val = i / 10\n",
    "    filename = f'./data/QA/qwen3-coder-A3B-instruct/qa_dataset_temp_{temp_val:.1f}.json'\n",
    "    file_paths[temp_val] = filename\n",
    "\n",
    "all_data = []\n",
    "for temp, path in file_paths.items():\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            for entry in data:\n",
    "                entry['temperature'] = temp # 출처(Temp) 표시\n",
    "            all_data.extend(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ 경고: 파일을 찾을 수 없습니다 - {path}\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# 2. 임베딩 및 유사도 계산 (Embedding & Similarity)\n",
    "okt = Okt() \n",
    "\n",
    "def korean_tokenizer(text): # 토크나이저 함수 정의 (명사, 동사, 형용사만 추출하거나 형태소 단위로 쪼갬)\n",
    "    # 형태소 분석 결과에서 '형태소(form)'만 추출\n",
    "    tokens = okt.morphs(text)\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=korean_tokenizer,\n",
    "    token_pattern=None,\n",
    "    use_idf=False,\n",
    "    norm='l2'        \n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df['question']) # 질문만 임베딩 진행\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 3. 중복 제거 및 선별\n",
    "visited = set()\n",
    "deduplicated_data = []\n",
    "duplicate_groups = []  # 확인용 리스트\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    \n",
    "    similar_indices = np.where(cosine_sim[i] > threshold)[0]\n",
    "    cluster = df.iloc[similar_indices].to_dict('records')\n",
    "    \n",
    "    # 중복 그룹 저장 (클러스터 크기가 1보다 큰 경우)\n",
    "    if len(cluster) > 1:\n",
    "        group_info = {\n",
    "            \"group_id\": len(duplicate_groups) + 1,\n",
    "            \"count\": len(cluster),\n",
    "            \"questions\": cluster\n",
    "        }\n",
    "        duplicate_groups.append(group_info)\n",
    "    \n",
    "    for idx in similar_indices:\n",
    "        visited.add(idx)\n",
    "    \n",
    "    # 대표 질문 선정 (Temperature가 가장 낮은 것)\n",
    "    best_candidate = sorted(cluster, key=lambda x: x['temperature'])[0]\n",
    "    deduplicated_data.append(best_candidate)\n",
    "\n",
    "# 4. 결과 저장\n",
    "output_df = pd.DataFrame(deduplicated_data)\n",
    "\n",
    "# 4-1. JSONL 포맷 저장 (한 줄에 하나씩)\n",
    "output_df.to_json('deduplicated_dataset.jsonl', orient='records', lines=True, force_ascii=False, indent=4)\n",
    "\n",
    "# 4-2. JSON 리스트 포맷 저장 (전체가 [ ]로 감싸짐)\n",
    "output_df.to_json('deduplicated_dataset.json', orient='records', lines=False, force_ascii=False, indent=4)\n",
    "\n",
    "# 4-3. 중복 그룹 검토용 저장\n",
    "with open('duplicate_groups_check.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(duplicate_groups, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# 5. 요약 출력\n",
    "print(f\"전체 데이터 수: {len(df)}\")\n",
    "print(f\"중복 제거 후: {len(output_df)}\")\n",
    "print(f\"발견된 중복 그룹 수: {len(duplicate_groups)}\")\n",
    "print(f\"파일 저장 완료:\")\n",
    "print(f\"   1. deduplicated_dataset.jsonl (라인별)\")\n",
    "print(f\"   2. deduplicated_dataset.json (리스트형)\")\n",
    "print(f\"   3. duplicate_groups_check.json (중복그룹확인)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f32b5a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 개수: 621개\n",
      "------------------------------\n",
      "simple: 329개\n",
      "procedural: 174개\n",
      "negative: 118개\n"
     ]
    }
   ],
   "source": [
    "# 카테고리별 개수 출력\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = 'deduplicated_dataset.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 데이터가 리스트인 경우 바로 사용하도록 수정\n",
    "if isinstance(data, list):\n",
    "    items = data\n",
    "else:\n",
    "    # 만약 딕셔너리 구조라면 기존 방식 시도\n",
    "    items = data.get('fullContent', [])\n",
    "\n",
    "# 카테고리만 추출하여 개수 세기\n",
    "categories = [item.get('category') for item in items if isinstance(item, dict) and 'category' in item]\n",
    "category_counts = Counter(categories)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"총 데이터 개수: {len(items)}개\")\n",
    "print(\"-\" * 30)\n",
    "for category, count in category_counts.most_common():\n",
    "    print(f\"{category}: {count}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f139d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정렬 결과 확인 (상위 5개):\n",
      "File: 01.txt | Category: simple | Question: 고용계약서는 누구에게 제출해야 합니까?\n",
      "File: 01.txt | Category: simple | Question: 고용계약서는 누구와 체결됩니까?\n",
      "File: 01.txt | Category: simple | Question: 고용계약서는 어떤 사항들을 포함해야 합니까?\n",
      "File: 01.txt | Category: simple | Question: 고용계약서는 언제부터 적용되나요?\n",
      "File: 01.txt | Category: procedural | Question: 고용계약서를 작성한 후 직원이 서명해야 하나요?\n",
      "\n",
      "최종 정렬된 데이터가 'sorted_dataset_v2.json' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# json 데이터 정렬\n",
    "import json\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "file_path = 'deduplicated_dataset.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 데이터가 리스트인지 딕셔너리인지 확인 후 리스트 추출\n",
    "items = data if isinstance(data, list) else data.get('fullContent', [])\n",
    "\n",
    "# 2. 카테고리 우선순위 정의\n",
    "category_order = {\n",
    "    'simple': 1,\n",
    "    'procedural': 2,\n",
    "    'negative': 3\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# 3. 정렬 수행 (3단계)\n",
    "# 1순위: source_file (파일명 오름차순)\n",
    "# 2순위: category (지정된 순서)\n",
    "# 3순위: question (가나다순 - instruction 역할)\n",
    "sorted_items = sorted(items, key=lambda x: (\n",
    "    x.get('source_file', ''),                  # 1. 파일명\n",
    "    category_order.get(x.get('category'), 99), # 2. 카테고리\n",
    "    x.get('question', '')                      # 3. 질문(instruction) 가나다순\n",
    "))\n",
    "\"\"\"\n",
    "\n",
    "# 2순위 제거\n",
    "sorted_items = sorted(items, key=lambda x: (\n",
    "    x.get('source_file', ''),                  # 1. 파일명\n",
    "    x.get('question', '')                      # 3. 질문(instruction) 가나다순\n",
    "))\n",
    "\n",
    "\n",
    "# 4. 결과 확인 (상위 5개 출력)\n",
    "print(\"정렬 결과 확인 (상위 5개):\")\n",
    "for item in sorted_items[:5]:\n",
    "    print(f\"File: {item.get('source_file')} | Category: {item.get('category')} | Question: {item.get('question')}\")\n",
    "\n",
    "# 5. 파일로 저장\n",
    "output_path = 'sorted_dataset_v2.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_items, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n최종 정렬된 데이터가 '{output_path}' 파일로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rules-fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import os
import json
import re
import math
from tqdm import tqdm
from typing import List, Optional
import logging
import time
from datetime import datetime

# LangChain ê´€ë ¨ ê¸°ëŠ¥ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

"""
# [ì„¤ì •] ê²½ë¡œ ë° vLLM ì„œë²„ ì •ë³´
"""
# ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ì£¼ì†Œì™€ ì¶œë ¥ íŒŒì¼ëª…
DATA_DIR = "./data/chunking_chapters_len_preprocess_final"
OUTPUT_FILE = "./data/QA/qwen3-coder-A3B-instruct/qa_dataset.json" # ê¸°ë³¸ íŒŒì¼ëª… (ì˜¨ë„ì— ë”°ë¼ ì´ë¦„ ë³€ê²½ ì˜ˆì •)

# --- ë¡œê·¸ íŒŒì¼ëª… ëª…ëª… ê·œì¹™ ì ìš© ---
# 1. í˜„ì¬ ì‹œê°ì„ YYYYMMDD_HHMMSS í˜•ì‹ìœ¼ë¡œ í¬ë§·
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
# 2. ë¡œê·¸ íŒŒì¼ëª… ì„¤ì •: qa_extraction_YYYYMMDD_HHMMSS.log
LOG_FILE = f"./logs/03_qa_extraction_{timestamp}.log"
# -----------------------------------

# vLLM ì„œë²„ ì •ë³´
OPENAI_BASE_URL = "http://localhost:8001/v1" # vLLM ì„œë²„ ì£¼ì†Œ, ì•ìœ¼ë¡œ ë³´ê³  ë°”ê¾¸ë©´ ë ë“¯?
OPENAI_API_KEY = "EMPTY" # ë¡œì»¬ì—ì„œ ì‹¤í–‰í•  ê±°ë¼ ë¹„ì–´ìˆìŒ ì²˜ë¦¬
MODEL_NAME = "Qwen/Qwen3-Coder-30B-A3B-Instruct" # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„

# ìƒì„±í•  ë°ì´í„° ì¢…ë¥˜ ë° ê°œìˆ˜ ì´ 300ê°œ(150, 100, 50) -> ì¼ë‹¨ ì²˜ìŒì— ì˜ ìƒì„±ë˜ëŠ”ì§€ 6, 4, 2ë¡œ ì‹¤í–‰
TOTAL_TARGETS = {
    "simple": 150, # ë‹¨ìˆœ ì¡°íšŒí˜•
    "procedural": 100,# ì ˆì°¨í˜•
    "negative": 50, # ë¶€ì •í˜•
}

"""
# [ë¡œê·¸ ì„¤ì •] ë¡œê±° ì´ˆê¸°í™”
"""
# 1. ë¡œê±° ì´ˆê¸°í™”
logging.basicConfig(
    level=logging.INFO, # INFO ë ˆë²¨ ì´ìƒë§Œ ê¸°ë¡
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='w', encoding='utf-8'), # íŒŒì¼ë¡œ ì €ì¥ (ë®ì–´ì“°ê¸°), íŒŒì¼ ì´ë¦„ ìˆ˜ì •í•´ì•¼ í•¨
        logging.StreamHandler() # ì½˜ì†”ì—ë„ ì¶œë ¥
    ]
)
logger = logging.getLogger(__name__) # ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ìœ¼ë¡œ ë¡œê±° ìƒì„±
logger.info(f"ë¡œê¹… ì‹œì‘. ë¡œê·¸ íŒŒì¼ ê²½ë¡œ: {LOG_FILE}")

"""
# [êµ¬ì¡° ì •ì˜] llmì´ ì¶œë ¥í•´ì•¼ í•  JSON í˜•ì‹ì— ëŒ€í•œ ì •ì˜
"""
class QAItem(BaseModel): # ì„¤ê³„ë„
    category: str = Field(description="ì§ˆë¬¸ ìœ í˜• (simple, procedural, negative ì¤‘ í•˜ë‚˜)")
    question: str = Field(description="ìƒì„±ëœ ì§ˆë¬¸ í…ìŠ¤íŠ¸")
    answer: str = Field(description="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ í…ìŠ¤íŠ¸")
    source: str = Field(description="ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì¥ í˜¹ì€ 'ê·œì • ì—†ìŒ' ë“±ì˜ ê·¼ê±°")

# ìƒˆë¡œìš´ ìµœìƒìœ„ ëª¨ë¸: ì—¬ëŸ¬ ê°œì˜ QAItemì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë‹´ëŠ” êµ¬ì¡°
class QAList(BaseModel): # <--- ì¶”ê°€
    qa_pairs: List[QAItem] = Field(description="ìƒì„±ëœ ì§ˆë¬¸-ë‹µë³€ ìŒ ë¦¬ìŠ¤íŠ¸")

"""
# [ìœ í‹¸ë¦¬í‹°] íŒŒì¼ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
"""
def clean_text(text): # ì´ë¯¸ ì§„í–‰í–ˆì§€ë§Œ í˜¹ì‹œ ëª¨ë¥´ë‹ˆê¹Œ ì¶”ê°€
    """ë¶ˆí•„ìš”í•œ ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬"""
    return re.sub(r'\s+', ' ', text).strip()

def load_files():
    """í´ë” ë‚´ì˜ txt íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ë”•ì…”ë„ˆë¦¬ {íŒŒì¼ëª…: ë‚´ìš©} í˜•íƒœë¡œ ë°˜í™˜"""
    context_dict = {}

    # íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
    files = sorted([f for f in os.listdir(DATA_DIR) if f.endswith('.txt')])
    logger.info(f"ğŸ“ ë°ì´í„° ë¡œë“œ ì‹œì‘: {DATA_DIR}ì—ì„œ {len(files)}ê°œ íŒŒì¼ ë°œê²¬") # <-- ë¡œê¹…
    
    # ë”•ì…”ë„ˆë¦¬ë¡œ `íŒŒì¼ëª…: ë‚´ìš©`` í˜•íƒœë¡œ ì €ì¥
    for f_name in files:
        file_path = os.path.join(DATA_DIR, f_name)
        with open(file_path, "r", encoding="utf-8") as f:
            content = clean_text(f.read())
            if content: # ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                context_dict[f_name] = content
                logger.debug(f"   - {f_name}: {len(content)}ì ë¡œë“œ ì™„ë£Œ") # <-- ë””ë²„ê·¸ ë¡œê¹…
            else:
                logger.warning(f"   - {f_name}: ë‚´ìš©ì´ ë¹„ì–´ìˆì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.") # <-- ê²½ê³  ë¡œê¹…
    return context_dict

"""
# [Langchain ì„¤ì •] í”„ë¡¬í”„íŠ¸ + ëª¨ë¸ + Parser ì •ì˜
"""
def create_qa_chain(temperature: float): # <--- temperatureë¥¼ ì¸ìˆ˜ë¡œ ë°›ë„ë¡ ìˆ˜ì •
    logger.info(f"âš™ï¸ LangChain ì²´ì¸ ìƒì„± ì‹œì‘ (Temperature: {temperature:.1f})") # <-- ë¡œê¹…
    # 1. ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(
        base_url=OPENAI_BASE_URL,
        api_key=OPENAI_API_KEY,
        model_name=MODEL_NAME,
        temperature=temperature, # <--- ì¸ìˆ˜ë¡œ ë°›ì€ temperature ì‚¬ìš©
        max_tokens=4096,
    )

    # 2. íŒŒì„œ ì„¤ì •
    # parser = JsonOutputParser(pydantic_object=QAItem)
    parser = JsonOutputParser(pydantic_object=QAList)

    # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
    # ë¶€ì •í˜• answerê°€ ì˜ì–´ë¡œ ì¶œë ¥ë˜ëŠ”ì§€ í™•ì¸ í•„ìš”í•¨
    template_str = """You are an Expert HR Regulation Specialist and AI Dataset Generator.
Your goal is to generate **{num_to_generate} high-quality Korean Q&A pairs** to help employees understand company regulations.

### Context (Company Regulations):
{context}

### Strict Constraints:
1. **Language Consistency (CRITICAL)**: Both QUESTION and ANSWER must be **STRICTLY generated in Korean (í•œêµ­ì–´)**. Do NOT use English.
2. **Source of Truth**: You must generate questions and answers based **ONLY** on the provided context. Do not use outside knowledge or general labor laws.
3. **Handling Missing Info (Negative QA)**: If the instruction asks about a topic NOT in the context, you must logically verify its absence and answer in Korean: "ì œê³µëœ ê·œì •ì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤." Do NOT invent fake rules.
4. **No Subjective Interpretation**: Do not infer priority (e.g., "most important"), intent, or subjective value unless explicitly written in the text. Stick to the facts presented.
5. **Completeness**: The ANSWER must be a complete, polite Korean sentence (e.g., ~í•©ë‹ˆë‹¤, ~ì…ë‹ˆë‹¤).
6. **Clarity**: The QUESTION should be self-contained and clear without looking at the context.
7. **Source Format**: The `source` field must contain the **full text segment** (Article number + Title + Content) that supports the answer. Do not abbreviate.

### Task Requirements:
- **Type**: {type_desc}
- **Instruction**: {instruction} (if any)
- **Example Question**: {ex_q}
- **Example Answer**: {ex_a}
- **Language**: Korean (í•œêµ­ì–´)

### Output Format:
{format_instructions}

IMPORTANT: Return ONLY the JSON object properly formatted.
"""

    # ì´ì „ì— ì‚¬ìš©í–ˆë˜ í”„ë¡¬í”„íŠ¸: ë¶€ì •í˜• answerê°€ ì˜ì–´ë¡œ ì¶œë ¥ë˜ì„œ ë³€ê²½ ì§„í–‰
    # "Such information is not found in the regulations" or "It is not specified." ì´ ë¶€ë¶„ ë•Œë¬¸ì— ë¶€ì •í˜•ì—ì„œ ì˜ì–´ë¡œ ì¶œë ¥ë¨
    before_template_str = """You are an Expert HR Regulation Specialist and AI Dataset Generator.
Your goal is to generate **{num_to_generate} high-quality Korean Q&A pair** to help employees understand company regulations.

### Context (Company Regulations):
{context}

### Strict Constraints:
1. **Source of Truth**: You must generate questions and answers based **ONLY** on the provided context. Do not use outside knowledge or general labor laws.
2. **Handling Missing Info**: If the instruction asks about a topic NOT in the context, your answer must explicitly state that "Such information is not found in the regulations" or "It is not specified." Do NOT invent fake rules.
3. **Completeness**: The ANSWER must be a complete, polite Korean sentence.
4. **Clarity**: The QUESTION should be self-contained and clear without looking at the context.
5. **Language Consistency**: Both QUESTION and ANSWER must be **STRICTLY** generated in **Korean (í•œêµ­ì–´)**.

### Task Requirements:
- **Type**: {type_desc}
- **Instruction**: {instruction}
- **Example Question**: {ex_q}
- **Example Answer**: {ex_a}
- **Language**: Korean (í•œêµ­ì–´)

### Output Format:
{format_instructions}

IMPORTANT: Return ONLY the JSON object properly formatted.
"""

    prompt = PromptTemplate(
        template=template_str,
        input_variables=["context", "type_desc", "instruction", "ex_q", "ex_a", "num_to_generate"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )

    # 4. ì²´ì¸ ì—°ê²° (LCEL ë¬¸ë²•: í”„ë¡¬í”„íŠ¸ -> ëª¨ë¸ -> íŒŒì„œ)
    chain = prompt | llm | parser
    logger.info("âœ… LangChain ì²´ì¸ ìƒì„± ì™„ë£Œ") # <-- ë¡œê¹…
    return chain

"""
# [ì‹¤í–‰ ë¡œì§] ë°ì´í„° ìƒì„± ë° ì €ì¥
"""
# ìœ í˜•í¼ í”„ë¡¬í”„íŠ¸ ì„¤ì •ê°’
TYPE_CONFIG = {
    "simple": {
        "desc": "Simple Fact Retrieval",
        "instruction": "ê·œì •ì— ëª…ì‹œëœ ìˆ«ì, ë‚ ì§œ, ê¸°ê°„, ì •ì˜, ì¡°ê±´ì„ ë¬»ëŠ” ì§ˆë¬¸.",
        "ex_q": "ì‹ ê·œ ì±„ìš©ëœ ì§ì›ì€ ì±„ìš©ì¼ë¡œë¶€í„° ë©°ì¹  ì´ë‚´ì— ê´€ë ¨ ì„œë¥˜ë¥¼ ì œì¶œí•´ì•¼ í•©ë‹ˆê¹Œ?",
        "ex_a": "ì‹ ê·œ ì±„ìš©ëœ ì§ì›ì€ ì±„ìš© í›„ 10ì¼ ì´ë‚´ì— ì„œë¥˜ë¥¼ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤.",
    },
    "procedural": {
        "desc": "Procedural/Process",
        "instruction": "íŠ¹ì • ìƒí™©ì—ì„œì˜ ì ˆì°¨, ì œì¶œ ì„œë¥˜, ìŠ¹ì¸ ê³¼ì • ë“±ì„ ë¬»ëŠ” ì§ˆë¬¸.",
        "ex_q": "ì˜ˆë¹„êµ° íœ´ê°€ë¥¼ ì‹ ì²­í•˜ë ¤ë©´ ì–´ë–¤ ì ˆì°¨ë¥¼ ê±°ì³ì•¼ í•©ë‹ˆê¹Œ?",
        "ex_a": "ì˜ˆë¹„êµ° íœ´ê°€ë¥¼ ì‹ ì²­í•˜ë ¤ë©´ í›ˆë ¨ ì¦ë¹™ ì„œë¥˜ë¥¼ ì²¨ë¶€í•˜ì—¬ ìƒì‚¬ì˜ ìŠ¹ì¸ì„ ë°›ì€ í›„ ì¸ì‚¬ë‹´ë‹¹ìì—ê²Œ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤.",
    },
    "negative": {
        "desc": "Negative/Non-existent",
        "instruction": "ê·œì •ì— ì—†ëŠ” ë‚´ìš©ì„ ë¬»ê±°ë‚˜ í‹€ë¦° ì „ì œë¥¼ í™•ì¸í•˜ëŠ” ì§ˆë¬¸. ë‹µë³€ì€ 'ê·œì •ì— ì—†ìŠµë‹ˆë‹¤' ë“±ìœ¼ë¡œ ì‹œì‘.",
        "ex_q": "íšŒì‚¬ì—ì„œ ì§ì›ë“¤ì—ê²Œ ê¸°ìˆ™ì‚¬ë¥¼ ì œê³µí•©ë‹ˆê¹Œ?",
        "ex_a": "ì œê³µëœ ì·¨ì—…ê·œì¹™ì—ëŠ” ê¸°ìˆ™ì‚¬ ì œê³µì— ê´€í•œ ì¡°í•­ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
    }
}

# [ì‹¤í–‰ ë¡œì§] ë°ì´í„° ìƒì„± ë° ì €ì¥ (ìˆ˜ì •ë¨)
def generate_dataset(context_dict, chain):
    # ì „ì²´ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time_total = time.time()

    dataset = []
    total_len = sum(len(t) for t in context_dict.values())
    if total_len == 0:
        logger.error("ğŸ›‘ ìƒì„±í•  ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.") # <-- ì—ëŸ¬ ë¡œê¹…
        return []

    target_count = sum(TOTAL_TARGETS.values())
    logger.info(f"ğŸ“Š ë°ì´í„° ìƒì„± ì‹œì‘ (ì´ í…ìŠ¤íŠ¸: {total_len}ì, ëª©í‘œ: {target_count}ê°œ)") # <-- ë¡œê¹…
    
    for fname, text in context_dict.items():
        # íŒŒì¼ë³„ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time_file = time.time()

        ratio = len(text) / total_len
        logger.info("=======================================================")
        logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: [{fname}] ({len(text)}ì, ë¹„ìœ¨: {ratio:.2f})") # <-- ë¡œê¹…
        
        for cat, target in TOTAL_TARGETS.items():
            count = max(1, math.floor(target * ratio)) # ì´ë§Œí¼ì„ í•œ ë²ˆì— ìƒì„±
            config = TYPE_CONFIG[cat]
            
            # tqdmì„ ì œê±°í•˜ê³  íŒŒì¼/ìœ í˜•ë³„ë¡œ í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ë„ë¡ ë³€ê²½
            logger.info(f"   -> ìœ í˜•: {cat} / ëª©í‘œ: {count}ê°œ ì¼ê´„ ìƒì„± ìš”ì²­") # <-- ë¡œê¹… 
            
            try:
                result_obj = chain.invoke({
                    "context": text,
                    "type_desc": config['desc'],
                    "instruction": config['instruction'],
                    "ex_q": config['ex_q'],
                    "ex_a": config['ex_a'],
                    "num_to_generate": count # ìƒì„±í•  ê°œìˆ˜ ì „ë‹¬
                })
            except Exception as e:
                # ì‚¬ìš©ì ìš”ì²­(try-except ê¸ˆì§€)ì— ë”°ë¼ ì˜ˆì™¸ë¥¼ ì§ì ‘ ì²˜ë¦¬í•˜ì§€ëŠ” ì•Šì§€ë§Œ,
                # ë¡œê·¸ë¥¼ ë‚¨ê²¨ ì˜¤ë¥˜ ìƒí™©ì„ ê¸°ë¡
                logger.error(f"âŒ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - íŒŒì¼: {fname}, ìœ í˜•: {cat}. ì˜¤ë¥˜: {e}")
                continue # ë‹¤ìŒ ìœ í˜•ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.


            # ---  ERROR ë°©ì§€ ë° ë°ì´í„° ì¶”ì¶œ ë¡œì§  ---
            qa_data = []
            
            # 1. Pydantic ì¸ìŠ¤í„´ìŠ¤ì¼ ê²½ìš° (ê°€ì¥ ì´ìƒì ì¸ ê²½ìš°)
            if hasattr(result_obj, 'qa_pairs'):
                qa_data = result_obj.qa_pairs
            # 2. ë‹¨ìˆœ ë”•ì…”ë„ˆë¦¬ì¼ ê²½ìš° (AttributeErrorì˜ ì›ì¸)
            elif isinstance(result_obj, dict):
                qa_data = result_obj.get('qa_pairs', [])
            
            generated_count = len(qa_data)
            logger.info(f"   <- ìœ í˜•: {cat} / {generated_count}ê°œ ì¶”ì¶œë¨ (ëª©í‘œ: {count})") # <-- ë¡œê¹…
            
            # ì¶”ì¶œëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ìµœì¢… ë°ì´í„°ì…‹ì— ì¶”ê°€
            for item in qa_data:
                # itemì´ ë”•ì…”ë„ˆë¦¬(dict)ì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ, í‚¤ ì ‘ê·¼ì„ ì‚¬ìš©
                if isinstance(item, dict):
                    item['source_file'] = fname
                    dataset.append(item)
                # itemì´ Pydantic ê°ì²´ì¼ ê²½ìš° dictë¡œ ë³€í™˜ í›„ ì €ì¥
                elif hasattr(item, 'dict'):
                    item_dict = item.dict()
                    item_dict['source_file'] = fname
                    dataset.append(item_dict)

            # --- íŒŒì¼ë³„ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ---
            end_time_file = time.time()
            elapsed_time_file = end_time_file - start_time_file
            logger.info(f"â³ [{fname}, {cat}] ì²˜ë¦¬ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {elapsed_time_file:.2f}ì´ˆ")
            # -----------------------------

        # --- ì „ì²´ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ---
        end_time_total = time.time()
        elapsed_time_total = end_time_total - start_time_total
        logger.info(f"âœ… ë°ì´í„° ìƒì„± ì™„ë£Œ. ì´ {len(dataset)}ê°œì˜ QA ìŒì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info(f"â±ï¸ **ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± ì´ ì†Œìš” ì‹œê°„: {elapsed_time_total:.2f}ì´ˆ**")
        # ---------------------------
    return dataset

"""
# ë©”ì¸ ì‹¤í–‰ë¶€
"""
if __name__ == "__main__":
    # --- ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œì‘ ì‹œê°„ ê¸°ë¡ ---
    script_start_time = time.time()
    # ----------------------------------------
    
    # 1. íŒŒì¼ ë¡œë“œ
    context_data = load_files()
    
    if not context_data:
        logger.error("ğŸ›‘ Context ë°ì´í„°ê°€ ì—†ì–´ ë°ì´í„° ìƒì„±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    else:
        # ì˜¨ë„ ì„¤ì • (0.0ë¶€í„° 1.0ê¹Œì§€ 0.1ì”© ì¦ê°€)
        # round(i * 0.1, 1)ì„ ì‚¬ìš©í•˜ì—¬ ë¶€ë™ ì†Œìˆ˜ì  ì˜¤ë¥˜ ë°©ì§€
        temperatures = [round(i * 0.1, 1) for i in range(2, 11)]

        logger.info(f"ì´ {len(temperatures)}ê°œì˜ ì˜¨ë„ ì„¤ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤: {temperatures}")
        
        for temp in temperatures:
            logger.info("=======================================================")
            logger.info(f"       DATA GENERATION START - TEMPERATURE: {temp:.1f}")
            logger.info("=======================================================")

            # 2. ì²´ì¸ ìƒì„± (ë³€ê²½ëœ ì˜¨ë„ ì ìš©)   
            qa_chain = create_qa_chain(temp)

            # 3. ë°ì´í„° ìƒì„±
            final_data = generate_dataset(context_data, qa_chain)

            # 4. ì €ì¥ íŒŒì¼ëª… ë³€ê²½ (ì˜¨ë„ ê°’ í¬í•¨)
            # ì˜ˆ: ./data/QA/.../qa_dataset_temp_0.1.json
            base_path, ext = os.path.splitext(OUTPUT_FILE)
            temp_output_file = f"{base_path}_temp_{temp:.1f}{ext}"
            
            # 5. ì €ì¥
            with open(temp_output_file, "w", encoding="utf-8") as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"[SUCCESS] ì´ {len(final_data)}ê°œì˜ ë°ì´í„°ì…‹ì´ '{temp_output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


        # --- ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ---
        script_end_time = time.time()
        script_elapsed_time = script_end_time - script_start_time
        logger.info("=== ëª¨ë“  ì˜¨ë„ ì„¤ì •ì— ëŒ€í•œ ë°ì´í„° ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ===")
        logger.info(f"ğŸš€ **ìŠ¤í¬ë¦½íŠ¸ ì „ì²´ ì‹¤í–‰ ì‹œê°„ (ì‹œì‘~ì¢…ë£Œ): {script_elapsed_time:.2f}ì´ˆ**")
        # ------------------------------------------  
        